{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scraping with requests + beautiful soup (we can't procede with this method since the webpage is dynamic and we can't fetch the main content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = 'https://www.parliament.bg/bg/plenaryst/ns/55/ID/10940'\n",
    "cert_path = r\"C:\\Users\\ivank\\Desktop\\Parliament certs\\parliament.pem\"\n",
    "headers = {'User-Agent' : 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36'} \n",
    "\n",
    "\n",
    "response = requests.get(url, verify=cert_path, headers = headers)\n",
    "#soup = BeautifulSoup(response.text, 'html')\n",
    "\n",
    "\n",
    "print(response.status_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scraping with selenium"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import re\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a list of URLs from the range between two urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def url_list(url1,url2):\n",
    "\n",
    "    urls = []\n",
    "    url1_num = int(url1.split('/')[8])\n",
    "    url2_num = int(url2.split('/')[8])\n",
    "\n",
    "    while url1_num < url2_num + 1:\n",
    "        url_combined = 'https://www.parliament.bg/bg/plenaryst/ns/55/ID/' + str(url1_num)\n",
    "        urls.append(url_combined)\n",
    "        url1_num +=1\n",
    "\n",
    "    print('Number of URLs to scrape: ' + str(len(urls)))\n",
    "    return urls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regular Expressions to parse different Elements from Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def end_of_hearing(text):\n",
    "    end_of_hearing_pattern = re.compile(r'\\d\\d,\\d\\d\\sч.\\)\\n{2,4}[А-Я][а-я]+')\n",
    "    match = end_of_hearing_pattern.search(text)\n",
    "    return match\n",
    "\n",
    "def end_position_hearing(text):\n",
    "    end_of_hearing_pattern = re.compile(r'\\d\\d,\\d\\d\\sч.\\)\\n{2,4}[А-Я][а-я]+')\n",
    "    match = end_of_hearing_pattern.search(text)\n",
    "    end_position = match.start()\n",
    "    return end_position"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Webpage of Parliament.bg and extract the text corpus. \n",
    "Setup Crawler with an Explicit waiting strategy (upon loading the xpath containing the text corpus). Fetch the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scraper (urls,explicit_wait_seconds,poll_frequency):\n",
    "\n",
    "    chrome_driver_path = r\"C:\\Program Files (x86)\\chromedriver-win64\\chromedriver.exe\"\n",
    "    cService = webdriver.ChromeService(chrome_driver_path)\n",
    "    driver = webdriver.Chrome(service = cService)\n",
    "\n",
    "    texts = []\n",
    "    successful_urls = []\n",
    "    unsuccessful_urls = []\n",
    "    unsuccessful_messages = []\n",
    "\n",
    "    for url in urls:\n",
    "\n",
    "        driver.get(url)\n",
    "\n",
    "        try: \n",
    "            WebDriverWait(driver, explicit_wait_seconds,poll_frequency).until(EC.presence_of_element_located((By.XPATH, '/html/body/div/main/div/div/div[2]/div[1]/div/div[3]')))\n",
    "            corpus = driver.find_element(By.ID, 'app')\n",
    "\n",
    "            if end_of_hearing(corpus.text) is None:\n",
    "\n",
    "                unsuccessful_urls.append(url)\n",
    "                unsuccessful_messages.append('Initial Xpath located for Url but corpus is empty')\n",
    "\n",
    "            else:\n",
    "\n",
    "                successful_urls.append(url)\n",
    "                texts.append(corpus.text)\n",
    "        \n",
    "        except: \n",
    "            WebDriverWait(driver, explicit_wait_seconds, poll_frequency).until(EC.presence_of_element_located((By.XPATH, '//*[@id=\"app\"]/main/div/div/div[2]/div[1]/div/div[2]')))\n",
    "            corpus = driver.find_element(By.ID, 'app')\n",
    "\n",
    "            if end_of_hearing(corpus.text) is None:\n",
    "\n",
    "                unsuccessful_urls.append(url)\n",
    "                unsuccessful_messages.append('Error triggered: Initial Xpath was not located and no corpus was found')\n",
    "\n",
    "            else:\n",
    "\n",
    "                successful_urls.append(url)\n",
    "                texts.append(corpus.text)\n",
    "    \n",
    "    scraper_dict = {\n",
    "        'texts' : texts,\n",
    "        'successful_urls' : successful_urls,\n",
    "        'unsuccessful_urls' : unsuccessful_urls,\n",
    "        'unsuccessful_messages' : unsuccessful_messages\n",
    "    }\n",
    "\n",
    "    print('Number of scraped URLs: ' + str(len(successful_urls)) + ' (' + str(round(100*len(successful_urls)/len(urls),2)) +'% Success)' )\n",
    "\n",
    "    return scraper_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get Basic Attributes, which will be added to each statement (Assembly Number, Hearing Number, Date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def general_parser (text):\n",
    "\n",
    "    assembly_pattern = re.compile(r'[А-Я]+\\sИ\\s[А-Я]+\\sНАРОДНО\\sСЪБРАНИЕ|[А-Я]+\\sНАРОДНО\\sСЪБРАНИЕ') \n",
    "    session_pattern = re.compile(r'[А-Я]+\\sИ\\s[А-Я]+\\sСЕСИЯ|[А-Я]+\\sСЕСИЯ')\n",
    "    hearing_pattern = re.compile(r'([А-Я]+\\sИ\\s[А-Я]+\\s[А-Я]+\\sЗАСЕДАНИЕ|[А-Я]+\\sИ\\s[А-Я]+\\sЗАСЕДАНИЕ|[А-Я]+\\s[А-Я]+\\sЗАСЕДАНИЕ|[А-Я]+\\sЗАСЕДАНИЕ)') \n",
    "\n",
    "\n",
    "    assembly_matches = assembly_pattern.search(text)\n",
    "\n",
    "    try:\n",
    "        session_matches = session_pattern.search(text)\n",
    "    except:\n",
    "        session_matches = 'False'\n",
    "\n",
    "\n",
    "    if session_matches == 'False':\n",
    "\n",
    "        hearing_matches = hearing_pattern.search(text[session_matches.end():])\n",
    "    else:\n",
    "        hearing_matches = hearing_pattern.search(text[assembly_matches.end():])\n",
    "\n",
    "\n",
    "    assembly = assembly_matches.group().title()\n",
    "    hearing = hearing_matches.group().title()\n",
    "\n",
    "    pattern_date = re.compile(r'(\\d{2}).(\\d{2}).(\\d{4})') # find the date of the session\n",
    "    matches_date = pattern_date.search(text)\n",
    "    year = matches_date.group(3)\n",
    "    month = matches_date.group(2)\n",
    "    day = matches_date.group(1)\n",
    "    date = year + '.' + month + '.' + day\n",
    "\n",
    "    general_info_dict = {\n",
    "        \n",
    "        'assembly': assembly,\n",
    "        'hearing': hearing,\n",
    "        'date': date\n",
    "                         }\n",
    "\n",
    "    return general_info_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get Statements, politicians first and last names and political parties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def statements_parser (text,url):\n",
    "\n",
    "    general_info_dict = general_parser(text)\n",
    "    pattern_statements = re.compile(r'([А-Я]+\\s)?([А-Я]+\\s)([А-Я]+)(:|\\s\\((.+)\\):)') # find  first name + last name + political party + statement\n",
    "    matches = pattern_statements.finditer(text)\n",
    "    match_position = pattern_statements.finditer(text)\n",
    "\n",
    "    first_names = []\n",
    "    last_names = []\n",
    "    political_parties_raw = []\n",
    "    assembly_roles = []\n",
    "    statements = []\n",
    "    end_positions = []\n",
    "    start_positions = []\n",
    "    assemblies = []\n",
    "    hearings = []\n",
    "    dates = []\n",
    "    urls = []\n",
    "\n",
    "\n",
    "    for index in match_position:\n",
    "\n",
    "        first_names.append(index.group(2).title())\n",
    "        last_names.append(index.group(3).title())\n",
    "        political_parties_raw.append(index.group(4))\n",
    "        end_positions.append(index.end())\n",
    "        start_positions.append(index.start())\n",
    "\n",
    "        if index.group(1) is None:\n",
    "            assembly_roles.append('Политик')\n",
    "        else:\n",
    "            assembly_roles.append(index.group(1).title())\n",
    "\n",
    "\n",
    "    i=0\n",
    "    \n",
    "    while  i < len(end_positions):\n",
    "\n",
    "        assemblies.append(general_info_dict.get('assembly'))\n",
    "        hearings.append(general_info_dict.get('hearing'))\n",
    "        dates.append(general_info_dict.get('date'))\n",
    "        urls.append(url)\n",
    "        i+=1\n",
    "\n",
    "    i=0   \n",
    "    \n",
    "    end_position = end_position_hearing(text)\n",
    "\n",
    "    for match in matches:\n",
    "\n",
    "        if i == len(end_positions) - 1:\n",
    "            \n",
    "            clean_statement = text[end_positions[i]:end_position].replace('\\n', ' ').strip().translate({ord(i): None for i in '('}).strip()\n",
    "            statements.append(clean_statement)\n",
    "            break\n",
    "        else:\n",
    "            clean_statement = text[end_positions[i]:start_positions[i+1]].replace('\\n', ' ').strip()\n",
    "            statements.append(clean_statement)\n",
    "            i+=1\n",
    "\n",
    "\n",
    "    political_parties = []\n",
    "    speaking_locations = []\n",
    "\n",
    "    for party in political_parties_raw:\n",
    "\n",
    "        if party == ':':\n",
    "            political_parties.append('Председателски Орган')\n",
    "            speaking_locations.append('От Трибуната')\n",
    "        else:\n",
    "            if ', от' in party:\n",
    "                clean = party.translate({ord(i): None for i in '():'}).strip()\n",
    "                clean_split = clean.split(', ')\n",
    "                political_parties.append(clean_split[0])\n",
    "                speaking_locations.append(clean_split[1].title())\n",
    "\n",
    "            elif 'встрани от микрофоните' in party:\n",
    "                speaking_locations.append('От Място')\n",
    "                political_parties.append('')\n",
    "\n",
    "            else:\n",
    "                clean = party.translate({ord(i): None for i in '():'}).strip()\n",
    "                political_parties.append(clean)\n",
    "                speaking_locations.append('От Трибуната')\n",
    "\n",
    "\n",
    "    statements_dict = {\n",
    "        'Народно Събрание': assemblies,\n",
    "        'Заседание': hearings, \n",
    "        'Дата': dates, \n",
    "        'Позиция в Парламента': assembly_roles,\n",
    "        'Първо Име': first_names,\n",
    "        'Фамилно Име': last_names,\n",
    "        'Партия': political_parties,\n",
    "        'Говорил От': speaking_locations,\n",
    "        'Изказване': statements,\n",
    "        'Начална Позиция на Изказване': start_positions,\n",
    "        'Крайна Позиция на Изказване': end_positions,\n",
    "        'Линк към изказване': urls\n",
    "    }\n",
    "\n",
    "    return statements_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a Pandas df from the gathered Attributes and save it to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_df (statements_dict):\n",
    "\n",
    "    df = pd.DataFrame.from_dict(statements_dict)\n",
    "    df.to_csv(r'C:\\Users\\ivank\\Desktop\\Scraper\\Hearings\\{date}_{assembly}.csv'.format(assembly= statements_dict.get('Народно Събрание')[0],date= statements_dict.get('Дата')[0]), encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a function Iterating through texts and mapping texts to CSV and subsequently saving them "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parser (scraper_dict):\n",
    "\n",
    "    texts = scraper_dict.get('texts')\n",
    "    urls = scraper_dict.get('successful_urls')\n",
    "    failed_scraping_urls = scraper_dict.get('unsuccessful_urls')\n",
    "    failed_scraping_messages = scraper_dict.get('unsuccessful_messages')\n",
    "\n",
    "    i = 0          \n",
    "    failed_mapping_urls = []\n",
    "    failed_mapping_messages = []\n",
    "\n",
    "    for text,url in zip(texts,urls):\n",
    "\n",
    "\n",
    "        try:\n",
    "            statements_dict = statements_parser(text,url)\n",
    "            save_df(statements_dict)\n",
    "\n",
    "        except:\n",
    "            failed_mapping_urls.append(url)\n",
    "            failed_mapping_messages.append('The parsing failed')\n",
    "        \n",
    "\n",
    "        i+=1\n",
    "\n",
    "\n",
    "    done_count = len(urls) - len(failed_mapping_urls)\n",
    "    success_rate = str(round(100*done_count / len(texts),2))\n",
    "\n",
    "    print('Successfully parsed and saved ' + str(done_count) + ' Texts (' + success_rate + '% Success)' )\n",
    "\n",
    "    failed_urls = failed_mapping_urls + failed_scraping_urls\n",
    "    failed_messages = failed_mapping_messages + failed_scraping_messages\n",
    "\n",
    "    failed_dict = { 'Url': failed_urls,\n",
    "                    'Message': failed_messages\n",
    "    }\n",
    "\n",
    "    df_failed =pd.DataFrame.from_dict(failed_dict)\n",
    "    df_failed.to_csv(r'C:\\Users\\ivank\\Desktop\\Scraper\\Failed Reports\\Failed_Report.csv', encoding='utf-8-sig')\n",
    "    \n",
    "    return\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine all methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parliament_scraper (url1,url2, explicit_wait_seconds, poll_frequency):\n",
    "\n",
    "    urls = url_list(url1,url2)\n",
    "    scraper_dict = scraper(urls,explicit_wait_seconds,poll_frequency)\n",
    "    parser(scraper_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of URLs to scrape: 12\n",
      "Number of scraped URLs: 12 (100.0% Success)\n",
      "Successfully parsed and saved 12 Texts (100.0% Success)\n"
     ]
    }
   ],
   "source": [
    "url1 = 'https://www.parliament.bg/bg/plenaryst/ns/55/ID/10958'\n",
    "url2 = 'https://www.parliament.bg/bg/plenaryst/ns/55/ID/10969'\n",
    "\n",
    "explicit_wait_seconds = 10\n",
    "poll_frequency = 2\n",
    "\n",
    "\n",
    "parliament_scraper(url1, url2, explicit_wait_seconds, poll_frequency)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
