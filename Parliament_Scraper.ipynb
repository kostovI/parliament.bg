{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0.0 Packages Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.0 Define Parameters\n",
    "\n",
    "### 1.1 Define a function to create a List of target URLs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def url_list(url1,url2=None):\n",
    "\n",
    "    url_base = 'https://www.parliament.bg/bg/plenaryst/ns/55/ID/'\n",
    "\n",
    "    if url2 == None:\n",
    "        print('Number of URLs to scrape: 1')\n",
    "        urls = [url1]\n",
    "        return urls\n",
    "\n",
    "    else:\n",
    "\n",
    "        urls = []\n",
    "        url1_num = int(url1.split('/')[8])\n",
    "        url2_num = int(url2.split('/')[8])\n",
    "\n",
    "        while url1_num < url2_num + 1:\n",
    "            url_combined = url_base + str(url1_num)\n",
    "            urls.append(url_combined)\n",
    "            url1_num +=1\n",
    "\n",
    "        print('Number of URLs to scrape: ' + str(len(urls)))\n",
    "        return urls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.0 Scraping with Selenium\n",
    "\n",
    "### 2.1 Use Regular Expressions to identify the end of a Parliament Hearing (will be used as criteria for a successful extraction of a text body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Regular Expressions to parse different Elements from Corpus\n",
    "\n",
    "def end_of_hearing(text):\n",
    "    end_of_hearing_pattern = re.compile(r'\\d\\d,\\d\\d\\sч.\\)\\n{2,4}[А-Я][а-я]+')\n",
    "    match = end_of_hearing_pattern.search(text)\n",
    "    return match\n",
    "\n",
    "def end_position_hearing(text):\n",
    "    end_of_hearing_pattern = re.compile(r'\\d\\d,\\d\\d\\sч.\\)\\n{2,4}[А-Я][а-я]+')\n",
    "    match = end_of_hearing_pattern.search(text)\n",
    "    end_position = match.start()\n",
    "    return end_position"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Set up Chrome Driver "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chrome_driver ():\n",
    "\n",
    "    options = webdriver.ChromeOptions()\n",
    "\n",
    "    #Optimize the the driver\n",
    "    options.add_argument(\"--headless=new\")          # Run headless (no GUI)\n",
    "    options.add_argument(\"--no-sandbox\")            # Disable sandbox for WSL/Docker\n",
    "    options.add_argument(\"--disable-dev-shm-usage\") # Avoid shared memory crashes in WSL/Docker\n",
    "\n",
    "    driver = webdriver.Chrome(options=options)\n",
    "\n",
    "    return driver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Define Scraping Strategy and Conditions, create a dict with texts and error messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setup Crawler with an Explicit waiting strategy Fetch the corpus and map it to a dict\n",
    "\n",
    "def scraper (urls,explicit_wait_seconds=10,poll_frequency=2):\n",
    "\n",
    "    driver = chrome_driver()\n",
    "\n",
    "    texts = []\n",
    "    successful_urls = []\n",
    "    unsuccessful_urls = []\n",
    "    unsuccessful_messages = []\n",
    "\n",
    "    for url in tqdm(urls,desc='Scraping Hearings'):\n",
    "        time.sleep(0.1)\n",
    "\n",
    "        driver.get(url)\n",
    "\n",
    "        try:\n",
    "            WebDriverWait(driver, explicit_wait_seconds,poll_frequency).until(EC.presence_of_element_located((By.XPATH, '/html/body/div/main/div/div/div[2]/div[1]/div/div[3]')))\n",
    "            corpus = driver.find_element(By.ID, 'app')\n",
    "\n",
    "            if end_of_hearing(corpus.text) is None:\n",
    "                unsuccessful_urls.append(url)\n",
    "                unsuccessful_messages.append('Initial Xpath located for Url but corpus is empty')\n",
    "\n",
    "            else:\n",
    "                successful_urls.append(url)\n",
    "                texts.append(corpus.text)\n",
    "\n",
    "        except:\n",
    "            WebDriverWait(driver, explicit_wait_seconds, poll_frequency).until(EC.presence_of_element_located((By.XPATH, '//*[@id=\"app\"]/main/div/div/div[2]/div[1]/div/div[2]')))\n",
    "            corpus = driver.find_element(By.ID, 'app')\n",
    "\n",
    "            if end_of_hearing(corpus.text) is None:\n",
    "\n",
    "                unsuccessful_urls.append(url)\n",
    "                unsuccessful_messages.append('Error triggered: Initial Xpath was not located and no corpus was found')\n",
    "\n",
    "            else:\n",
    "\n",
    "                successful_urls.append(url)\n",
    "                texts.append(corpus.text)\n",
    "\n",
    "    scraper_dict = {\n",
    "        'texts' : texts,\n",
    "        'successful_urls' : successful_urls,\n",
    "        'unsuccessful_urls' : unsuccessful_urls,\n",
    "        'unsuccessful_messages' : unsuccessful_messages\n",
    "    }\n",
    "\n",
    "    print('Number of successfully scraped URLs: ' + str(len(successful_urls)) + ' (' + str(round(100*len(successful_urls)/len(urls),2)) +'%)' )\n",
    "\n",
    "    return scraper_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.0 Parsing texts with Regular Expressions\n",
    "### 3.1 Creation of Regular Expressions patterns for identification of attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get Basic Attributes, which will be added to each statement (Assembly Number, Hearing Number, Date)\n",
    "assembly_pattern = re.compile(r'[А-Я]+\\sИ\\s[А-Я]+\\sНАРОДНО\\sСЪБРАНИЕ|[А-Я]+\\sНАРОДНО\\sСЪБРАНИЕ')\n",
    "session_pattern = re.compile(r'[А-Я]+\\sИ\\s[А-Я]+\\sСЕСИЯ|[А-Я]+\\sСЕСИЯ')\n",
    "hearing_pattern = re.compile(r'([А-Я]+\\sИ\\s[А-Я]+\\s[А-Я]+\\sЗАСЕДАНИЕ|[А-Я]+\\sИ\\s[А-Я]+\\sЗАСЕДАНИЕ|[А-Я]+\\s[А-Я]+\\sЗАСЕДАНИЕ|[А-Я]+\\sЗАСЕДАНИЕ)')\n",
    "\n",
    "#find  first name + last name + political party + statement\n",
    "pattern_statements = re.compile(r'([А-Я]+\\s)?([А-Я]+\\s)([А-Я]+)(:|\\s\\((.+)\\):)')\n",
    "\n",
    "# find the date of the session\n",
    "pattern_date = re.compile(r'(\\d{2}).(\\d{2}).(\\d{4})')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Create Functions to Extract Information about Hearing (general and statement specific)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to extract general attributes for a hearing from a text body\n",
    "def general_parser (text):\n",
    "\n",
    "    assembly_matches = assembly_pattern.search(text)\n",
    "\n",
    "    try:\n",
    "        session_matches = session_pattern.search(text)\n",
    "    except:\n",
    "        session_matches = 'False'\n",
    "\n",
    "    if session_matches == 'False':\n",
    "\n",
    "        hearing_matches = hearing_pattern.search(text[session_matches.end():])\n",
    "    else:\n",
    "        hearing_matches = hearing_pattern.search(text[assembly_matches.end():])\n",
    "\n",
    "\n",
    "    assembly = assembly_matches.group().title()\n",
    "    hearing = hearing_matches.group().title()\n",
    "\n",
    "    matches_date = pattern_date.search(text)\n",
    "    year = matches_date.group(3)\n",
    "    month = matches_date.group(2)\n",
    "    day = matches_date.group(1)\n",
    "    date = year + '.' + month + '.' + day\n",
    "\n",
    "    general_info_dict = {\n",
    "        'assembly': assembly,\n",
    "        'hearing': hearing,\n",
    "        'date': date\n",
    "                         }\n",
    "\n",
    "    return general_info_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get Statements, politicians first and last names and political parties\n",
    "def statements_parser (text,url):\n",
    "\n",
    "    match_position = pattern_statements.finditer(text)\n",
    "    general_info_dict = general_parser(text)\n",
    "\n",
    "    first_names = []\n",
    "    last_names = []\n",
    "    political_parties_raw = []\n",
    "    assembly_roles = []\n",
    "    statements = []\n",
    "    start_positions_politician = []\n",
    "    end_positions_politician = []\n",
    "    assemblies = []\n",
    "    hearings = []\n",
    "    dates = []\n",
    "    urls = []\n",
    "\n",
    "    for index in match_position:\n",
    "\n",
    "        first_names.append(index.group(2).title())\n",
    "        last_names.append(index.group(3).title())\n",
    "        political_parties_raw.append(index.group(4))\n",
    "        end_positions_politician.append(index.end())\n",
    "        start_positions_politician.append(index.start())\n",
    "\n",
    "        if index.group(1) is None:\n",
    "            assembly_roles.append('Политик')\n",
    "        else:\n",
    "            assembly_roles.append(index.group(1).title())\n",
    "\n",
    "    number_statements = len(first_names)\n",
    "\n",
    "    i=0\n",
    "\n",
    "    while  i < number_statements:\n",
    "\n",
    "        assemblies.append(general_info_dict.get('assembly'))\n",
    "        hearings.append(general_info_dict.get('hearing'))\n",
    "        dates.append(general_info_dict.get('date'))\n",
    "        urls.append(url)\n",
    "        i+=1\n",
    "\n",
    "\n",
    "    start_position_statement = []\n",
    "    end_position_statement = []\n",
    "    last_hearing_position = end_position_hearing(text)\n",
    "\n",
    "    i = 0\n",
    "\n",
    "    while i < number_statements :\n",
    "\n",
    "        if i == len(first_names) - 1:\n",
    "            start_position_statement.append(end_positions_politician[i])\n",
    "            end_position_statement.append(last_hearing_position)\n",
    "\n",
    "            statement = text[end_positions_politician[i]:last_hearing_position]\n",
    "            clean_statement = statement.translate({ord(i): None for i in '('}).replace('\\n', ' ').strip()\n",
    "            statements.append(clean_statement)\n",
    "\n",
    "        else:\n",
    "            start_position_statement.append(end_positions_politician[i])\n",
    "            end_position_statement.append(start_positions_politician[i+1])\n",
    "\n",
    "            statement = text[end_positions_politician[i]:start_positions_politician[i+1]]\n",
    "            clean_statement = statement.replace('\\n', ' ').strip()\n",
    "            statements.append(clean_statement)\n",
    "\n",
    "        i+=1\n",
    "\n",
    "    political_parties = []\n",
    "    speaking_locations = []\n",
    "\n",
    "    for party in political_parties_raw:\n",
    "\n",
    "        if party == ':':\n",
    "            political_parties.append('Председателски Орган')\n",
    "            speaking_locations.append('От Трибуната')\n",
    "        else:\n",
    "            if ', от' in party:\n",
    "                clean = party.translate({ord(i): None for i in '():'}).strip()\n",
    "                clean_split = clean.split(', ')\n",
    "                political_parties.append(clean_split[0])\n",
    "                speaking_locations.append(clean_split[1].title())\n",
    "\n",
    "            elif 'встрани от микрофоните' in party:\n",
    "                speaking_locations.append('От Място')\n",
    "                political_parties.append('')\n",
    "\n",
    "            else:\n",
    "                clean = party.translate({ord(i): None for i in '():'}).strip()\n",
    "                political_parties.append(clean)\n",
    "                speaking_locations.append('От Трибуната')\n",
    "\n",
    "\n",
    "    statements_dict = {\n",
    "        'Народно Събрание': assemblies,\n",
    "        'Заседание': hearings,\n",
    "        'Дата': dates,\n",
    "        'Позиция в Парламента': assembly_roles,\n",
    "        'Първо Име': first_names,\n",
    "        'Фамилно Име': last_names,\n",
    "        'Партия': political_parties,\n",
    "        'Говорил От': speaking_locations,\n",
    "        'Изказване': statements,\n",
    "        'Начална Позиция на Изказване': start_position_statement,\n",
    "        'Крайна Позиция на Изказване': end_position_statement,\n",
    "        'Линк към изказване': urls\n",
    "    }\n",
    "\n",
    "    return statements_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.0 Creating and Saving Dataframes from Parsed Texts and Exporting Them\n",
    "\n",
    "### 4.1 Create Raw Data Directory with Subfolders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if raw_data directory exists and if not create one then create inside hearings folder and failed reports\n",
    "def create_directories ():\n",
    "\n",
    "    #Get current working directory and create hearings dir\n",
    "    current_working_dir = os.getcwd()\n",
    "    raw_data_dir = current_working_dir + '/raw_data'\n",
    "\n",
    "    #Create Raw Data Directory\n",
    "    if os.path.exists(raw_data_dir):\n",
    "        pass\n",
    "    else:\n",
    "        print(\"Raw Data Directory created.\")\n",
    "        os.mkdir(raw_data_dir)\n",
    "\n",
    "    os.chdir(raw_data_dir)\n",
    "\n",
    "    #Create Inside directory Hearings and Failed Reports\n",
    "    try:\n",
    "        os.mkdir('hearings')\n",
    "        os.mkdir('failed_reports')\n",
    "    except FileExistsError:\n",
    "        pass\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "    os.chdir(current_working_dir)\n",
    "\n",
    "#create subfolder for each assembly\n",
    "def sub_folder_creator (assembly):\n",
    "\n",
    "    #Get current working directory and create hearings dir\n",
    "    current_working_dir = os.getcwd()\n",
    "    hearings_dir = current_working_dir + '/raw_data/hearings'\n",
    "\n",
    "    os.chdir(hearings_dir)\n",
    "\n",
    "    try:\n",
    "        os.mkdir(assembly)\n",
    "        print(f\"Directory for {assembly}created.\")\n",
    "    except FileExistsError:\n",
    "        pass\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "    os.chdir(current_working_dir)\n",
    "\n",
    "def save_df (statements_dict):\n",
    "\n",
    "    #Get current working directory and create hearings dir\n",
    "    current_working_dir = os.getcwd()\n",
    "    hearings_dir = current_working_dir + '/raw_data/hearings'\n",
    "\n",
    "    assembly = statements_dict.get('Народно Събрание')[0]\n",
    "    date = statements_dict.get('Дата')[0]\n",
    "\n",
    "    saving_dir = hearings_dir + '/' + assembly + '/' + date + '.csv'\n",
    "\n",
    "    df = pd.DataFrame.from_dict(statements_dict)\n",
    "    df.to_csv(saving_dir, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a function Iterating through texts and mapping texts to CSV and subsequently saving them\n",
    "def mapper (scraper_dict):\n",
    "\n",
    "    texts = scraper_dict.get('texts')\n",
    "    urls = scraper_dict.get('successful_urls')\n",
    "    failed_scraping_urls = scraper_dict.get('unsuccessful_urls')\n",
    "    failed_scraping_messages = scraper_dict.get('unsuccessful_messages')\n",
    "\n",
    "\n",
    "    failed_mapping_urls = []\n",
    "    failed_mapping_messages = []\n",
    "    create_directories ()\n",
    "\n",
    "    for text,url in zip(texts,urls):\n",
    "\n",
    "        try:\n",
    "            statements_dict = statements_parser(text,url)\n",
    "            assembly = statements_dict.get('Народно Събрание')[0]\n",
    "            sub_folder_creator(assembly)\n",
    "            save_df(statements_dict)\n",
    "\n",
    "        except:\n",
    "            failed_mapping_urls.append(url)\n",
    "            failed_mapping_messages.append('The parsing failed')\n",
    "\n",
    "\n",
    "    done_count = len(urls) - len(failed_mapping_urls)\n",
    "    success_rate = str(round(100*done_count / len(texts),2))\n",
    "\n",
    "    print('Parsed and Saved ' + str(done_count) + ' Texts (' + success_rate + '% Success)' )\n",
    "\n",
    "    failed_urls = failed_mapping_urls + failed_scraping_urls\n",
    "    failed_messages = failed_mapping_messages + failed_scraping_messages\n",
    "\n",
    "    failed_dict = { 'Url': failed_urls,\n",
    "                    'Message': failed_messages\n",
    "    }\n",
    "\n",
    "    current_working_dir = os.getcwd()\n",
    "    failed_dir = current_working_dir + '/raw_data/failed_reports/failed_report_' + str(len(failed_mapping_urls))+ '.csv'\n",
    "\n",
    "    df_failed = pd.DataFrame.from_dict(failed_dict)\n",
    "    df_failed.to_csv(failed_dir, encoding='utf-8-sig')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.0 Function which combines all previous Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parliament_scraper (url1,url2=None):\n",
    "    urls = url_list(url1,url2)\n",
    "    scraper_dict = scraper(urls)\n",
    "    mapper(scraper_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example of Scraping Parliament Stenograms for the 51. Assembly until 19.12.2025\n",
    "\n",
    "### Script will get all hearings which happened between the selected hearings for url1 and url2 and save them as CSV files "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of URLs to scrape: 151\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping Hearings: 100%|██████████| 151/151 [11:54<00:00,  4.73s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of successfully scraped URLs: 143 (94.7%)\n",
      "Raw Data Directory created.\n",
      "Directory for Петдесет И Първо Народно Събраниеcreated.\n",
      "Parsed and Saved 143 Texts (100.0% Success)\n"
     ]
    }
   ],
   "source": [
    "#Link to First Hearing of the 51. Assembly, which started officially on 11.11.2024\n",
    "url1 = 'https://parliament.bg/bg/plenaryst/ns/55/ID/10940'\n",
    "#Link to Hearing on the 19.12.2025\n",
    "url2 = 'https://parliament.bg/bg/plenaryst/ns/55/ID/11090'\n",
    "\n",
    "parliament_scraper(url1,url2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "parliament.bg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
