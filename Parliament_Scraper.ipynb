{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scraping with requests + beautiful soup (we can't proceed with this method since the webpage is dynamic and we can't fetch the main content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = 'https://www.parliament.bg/bg/plenaryst/ns/55/ID/10940'\n",
    "cert_path = r\"C:\\Users\\ivank\\Desktop\\Parliament certs\\parliament.pem\"\n",
    "headers = {'User-Agent' : 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36'} \n",
    "\n",
    "\n",
    "response = requests.get(url, verify=cert_path, headers = headers)\n",
    "#soup = BeautifulSoup(response.text, 'html')\n",
    "\n",
    "\n",
    "print(response.status_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scraping with selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Packages\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import re\n",
    "import pandas as pd\n",
    "import os \n",
    "\n",
    "#Create a list of URLs from the range between two urls\n",
    "\n",
    "def url_list(url1,url2):\n",
    "\n",
    "    urls = []\n",
    "    url1_num = int(url1.split('/')[8])\n",
    "    url2_num = int(url2.split('/')[8])\n",
    "\n",
    "    while url1_num < url2_num + 1:\n",
    "        url_combined = 'https://www.parliament.bg/bg/plenaryst/ns/55/ID/' + str(url1_num)\n",
    "        urls.append(url_combined)\n",
    "        url1_num +=1\n",
    "\n",
    "    print('Number of URLs to scrape: ' + str(len(urls)))\n",
    "    return urls\n",
    "\n",
    "#Regular Expressions to parse different Elements from Corpus\n",
    "\n",
    "def end_of_hearing(text):\n",
    "    end_of_hearing_pattern = re.compile(r'\\d\\d,\\d\\d\\sч.\\)\\n{2,4}[А-Я][а-я]+')\n",
    "    match = end_of_hearing_pattern.search(text)\n",
    "    return match\n",
    "\n",
    "def end_position_hearing(text):\n",
    "    end_of_hearing_pattern = re.compile(r'\\d\\d,\\d\\d\\sч.\\)\\n{2,4}[А-Я][а-я]+')\n",
    "    match = end_of_hearing_pattern.search(text)\n",
    "    end_position = match.start()\n",
    "    return end_position\n",
    "\n",
    "#Load Webpage of Parliament.bg and extract the text corpus. \n",
    "#Setup Crawler with an Explicit waiting strategy (upon loading the xpath containing the text corpus). Fetch the corpus\n",
    "\n",
    "def scraper (urls,explicit_wait_seconds,poll_frequency,chrome_driver_path):\n",
    "\n",
    "    cService = webdriver.ChromeService(chrome_driver_path)\n",
    "    driver = webdriver.Chrome(service = cService)\n",
    "\n",
    "    texts = []\n",
    "    successful_urls = []\n",
    "    unsuccessful_urls = []\n",
    "    unsuccessful_messages = []\n",
    "\n",
    "    for url in urls:\n",
    "\n",
    "        driver.get(url)\n",
    "\n",
    "        try: \n",
    "            WebDriverWait(driver, explicit_wait_seconds,poll_frequency).until(EC.presence_of_element_located((By.XPATH, '/html/body/div/main/div/div/div[2]/div[1]/div/div[3]')))\n",
    "            corpus = driver.find_element(By.ID, 'app')\n",
    "\n",
    "            if end_of_hearing(corpus.text) is None:\n",
    "\n",
    "                unsuccessful_urls.append(url)\n",
    "                unsuccessful_messages.append('Initial Xpath located for Url but corpus is empty')\n",
    "\n",
    "            else:\n",
    "\n",
    "                successful_urls.append(url)\n",
    "                texts.append(corpus.text)\n",
    "        \n",
    "        except: \n",
    "            WebDriverWait(driver, explicit_wait_seconds, poll_frequency).until(EC.presence_of_element_located((By.XPATH, '//*[@id=\"app\"]/main/div/div/div[2]/div[1]/div/div[2]')))\n",
    "            corpus = driver.find_element(By.ID, 'app')\n",
    "\n",
    "            if end_of_hearing(corpus.text) is None:\n",
    "\n",
    "                unsuccessful_urls.append(url)\n",
    "                unsuccessful_messages.append('Error triggered: Initial Xpath was not located and no corpus was found')\n",
    "\n",
    "            else:\n",
    "\n",
    "                successful_urls.append(url)\n",
    "                texts.append(corpus.text)\n",
    "    \n",
    "    scraper_dict = {\n",
    "        'texts' : texts,\n",
    "        'successful_urls' : successful_urls,\n",
    "        'unsuccessful_urls' : unsuccessful_urls,\n",
    "        'unsuccessful_messages' : unsuccessful_messages\n",
    "    }\n",
    "\n",
    "    print('Number of scraped URLs: ' + str(len(successful_urls)) + ' (' + str(round(100*len(successful_urls)/len(urls),2)) +'% Success)' )\n",
    "\n",
    "    return scraper_dict\n",
    "\n",
    "#Get Basic Attributes, which will be added to each statement (Assembly Number, Hearing Number, Date)\n",
    "\n",
    "def general_parser (text):\n",
    "\n",
    "\n",
    "    assembly_pattern = re.compile(r'[А-Я]+\\sИ\\s[А-Я]+\\sНАРОДНО\\sСЪБРАНИЕ|[А-Я]+\\sНАРОДНО\\sСЪБРАНИЕ') \n",
    "    session_pattern = re.compile(r'[А-Я]+\\sИ\\s[А-Я]+\\sСЕСИЯ|[А-Я]+\\sСЕСИЯ')\n",
    "    hearing_pattern = re.compile(r'([А-Я]+\\sИ\\s[А-Я]+\\s[А-Я]+\\sЗАСЕДАНИЕ|[А-Я]+\\sИ\\s[А-Я]+\\sЗАСЕДАНИЕ|[А-Я]+\\s[А-Я]+\\sЗАСЕДАНИЕ|[А-Я]+\\sЗАСЕДАНИЕ)') \n",
    "\n",
    "\n",
    "    assembly_matches = assembly_pattern.search(text)\n",
    "\n",
    "    try:\n",
    "        session_matches = session_pattern.search(text)\n",
    "    except:\n",
    "        session_matches = 'False'\n",
    "\n",
    "\n",
    "    if session_matches == 'False':\n",
    "\n",
    "        hearing_matches = hearing_pattern.search(text[session_matches.end():])\n",
    "    else:\n",
    "        hearing_matches = hearing_pattern.search(text[assembly_matches.end():])\n",
    "\n",
    "\n",
    "    assembly = assembly_matches.group().title()\n",
    "    hearing = hearing_matches.group().title()\n",
    "\n",
    "    pattern_date = re.compile(r'(\\d{2}).(\\d{2}).(\\d{4})') # find the date of the session\n",
    "    matches_date = pattern_date.search(text)\n",
    "    year = matches_date.group(3)\n",
    "    month = matches_date.group(2)\n",
    "    day = matches_date.group(1)\n",
    "    date = year + '.' + month + '.' + day\n",
    "\n",
    "    general_info_dict = {\n",
    "        \n",
    "        'assembly': assembly,\n",
    "        'hearing': hearing,\n",
    "        'date': date\n",
    "                         }\n",
    "\n",
    "    return general_info_dict\n",
    "\n",
    "#Get Statements, politicians first and last names and political parties\n",
    "\n",
    "def statements_parser (text,url):\n",
    "\n",
    "    pattern_statements = re.compile(r'([А-Я]+\\s)?([А-Я]+\\s)([А-Я]+)(:|\\s\\((.+)\\):)') # find  first name + last name + political party + statement\n",
    "    match_position = pattern_statements.finditer(text)\n",
    "    general_info_dict = general_parser(text)\n",
    "\n",
    "    first_names = []\n",
    "    last_names = []\n",
    "    political_parties_raw = []\n",
    "    assembly_roles = []\n",
    "    statements = []\n",
    "    start_positions_politician = []\n",
    "    end_positions_politician = []\n",
    "    assemblies = []\n",
    "    hearings = []\n",
    "    dates = []\n",
    "    urls = []\n",
    "\n",
    "    for index in match_position:\n",
    "\n",
    "        first_names.append(index.group(2).title())\n",
    "        last_names.append(index.group(3).title())\n",
    "        political_parties_raw.append(index.group(4))\n",
    "        end_positions_politician.append(index.end())\n",
    "        start_positions_politician.append(index.start())\n",
    "\n",
    "        if index.group(1) is None:\n",
    "            assembly_roles.append('Политик')\n",
    "        else:\n",
    "            assembly_roles.append(index.group(1).title())\n",
    "\n",
    "    number_statements = len(first_names)\n",
    "\n",
    "    i=0\n",
    "\n",
    "    while  i < number_statements:\n",
    "\n",
    "        assemblies.append(general_info_dict.get('assembly'))\n",
    "        hearings.append(general_info_dict.get('hearing'))\n",
    "        dates.append(general_info_dict.get('date'))\n",
    "        urls.append(url)\n",
    "        i+=1\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    start_position_statement = []\n",
    "    end_position_statement = []\n",
    "    last_hearing_position = end_position_hearing(text)\n",
    "\n",
    "    i = 0 \n",
    "\n",
    "    while i < number_statements :\n",
    "\n",
    "        if i == len(first_names) - 1:\n",
    "            start_position_statement.append(end_positions_politician[i])\n",
    "            end_position_statement.append(last_hearing_position)\n",
    "            \n",
    "            statement = text[end_positions_politician[i]:last_hearing_position]\n",
    "            clean_statement = statement.translate({ord(i): None for i in '('}).replace('\\n', ' ').strip()\n",
    "            statements.append(clean_statement)\n",
    "            \n",
    "        else:\n",
    "            start_position_statement.append(end_positions_politician[i])\n",
    "            end_position_statement.append(start_positions_politician[i+1])\n",
    "\n",
    "            statement = text[end_positions_politician[i]:start_positions_politician[i+1]]\n",
    "            clean_statement = statement.replace('\\n', ' ').strip()\n",
    "            statements.append(clean_statement)\n",
    "            \n",
    "        i+=1\n",
    "\n",
    "    political_parties = []\n",
    "    speaking_locations = []\n",
    "\n",
    "    for party in political_parties_raw:\n",
    "\n",
    "        if party == ':':\n",
    "            political_parties.append('Председателски Орган')\n",
    "            speaking_locations.append('От Трибуната')\n",
    "        else:\n",
    "            if ', от' in party:\n",
    "                clean = party.translate({ord(i): None for i in '():'}).strip()\n",
    "                clean_split = clean.split(', ')\n",
    "                political_parties.append(clean_split[0])\n",
    "                speaking_locations.append(clean_split[1].title())\n",
    "\n",
    "            elif 'встрани от микрофоните' in party:\n",
    "                speaking_locations.append('От Място')\n",
    "                political_parties.append('')\n",
    "\n",
    "            else:\n",
    "                clean = party.translate({ord(i): None for i in '():'}).strip()\n",
    "                political_parties.append(clean)\n",
    "                speaking_locations.append('От Трибуната')\n",
    "\n",
    "\n",
    "    statements_dict = {\n",
    "        'Народно Събрание': assemblies,\n",
    "        'Заседание': hearings, \n",
    "        'Дата': dates, \n",
    "        'Позиция в Парламента': assembly_roles,\n",
    "        'Първо Име': first_names,\n",
    "        'Фамилно Име': last_names,\n",
    "        'Партия': political_parties,\n",
    "        'Говорил От': speaking_locations,\n",
    "        'Изказване': statements,\n",
    "        'Начална Позиция на Изказване': start_position_statement,\n",
    "        'Крайна Позиция на Изказване': end_position_statement,\n",
    "        'Линк към изказване': urls\n",
    "    }\n",
    "\n",
    "    return statements_dict\n",
    "\n",
    "#Create Subfolders and save the ready CSV Files there\n",
    "\n",
    "def path_exists (path):\n",
    "    try: \n",
    "        os.chdir(path)\n",
    "        return 'true'\n",
    "    except:\n",
    "        return 'The specified directory path does not exist!'\n",
    "\n",
    "def main_folder_creator (directory_path):\n",
    "    \n",
    "    directory_name = 'Scraper Results'\n",
    "\n",
    "    #create main folder directory\n",
    "    os.chdir(directory_path)\n",
    "\n",
    "    try:\n",
    "        os.mkdir(directory_name)\n",
    "        print(f\"Directory of scraper Results'{directory_path}\\{directory_name}' successfully created.\")\n",
    "    except FileExistsError:\n",
    "        pass\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "    #create subfolders Hearings and Failed Reports\n",
    "    folders_directory_path = os.path.join(directory_path, directory_name)\n",
    "    os.chdir(folders_directory_path)\n",
    "\n",
    "    try: \n",
    "        os.mkdir('Failed Reports')\n",
    "        os.mkdir('Hearings')\n",
    "    except FileExistsError:\n",
    "        pass  \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "#create subfolder for each assembly\n",
    "\n",
    "def sub_folder_creator (assembly,directory_path):\n",
    "\n",
    "    hearings_path = os.path.join(directory_path, 'Scraper Results','Hearings')\n",
    "    os.chdir(hearings_path)\n",
    "\n",
    "    try:\n",
    "        os.mkdir(assembly)\n",
    "        print(f\"Directory '{hearings_path}\\{assembly}' successfully created.\")\n",
    "    except FileExistsError:\n",
    "        pass\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "def save_df (statements_dict, directory_path):  \n",
    "\n",
    "    df = pd.DataFrame.from_dict(statements_dict)\n",
    "    folder_path = 'Scraper Results\\Hearings'\n",
    "    save_path = os.path.join(directory_path, folder_path,statements_dict.get('Народно Събрание')[0])\n",
    "    df.to_csv(save_path +'\\{date}.csv'.format(date= statements_dict.get('Дата')[0]), encoding='utf-8-sig')\n",
    "\n",
    "#Create a function Iterating through texts and mapping texts to CSV and subsequently saving them \n",
    "\n",
    "def parser (scraper_dict,directory_path):\n",
    "\n",
    "    texts = scraper_dict.get('texts')\n",
    "    urls = scraper_dict.get('successful_urls')\n",
    "    failed_scraping_urls = scraper_dict.get('unsuccessful_urls')\n",
    "    failed_scraping_messages = scraper_dict.get('unsuccessful_messages')\n",
    "\n",
    "         \n",
    "    failed_mapping_urls = []\n",
    "    failed_mapping_messages = []\n",
    "    main_folder_creator (directory_path)\n",
    "    \n",
    "    for text,url in zip(texts,urls):\n",
    "\n",
    "        try:\n",
    "            statements_dict = statements_parser(text,url)\n",
    "            assembly = statements_dict.get('Народно Събрание')[0]\n",
    "            sub_folder_creator(assembly,directory_path)\n",
    "            save_df(statements_dict,directory_path)\n",
    "\n",
    "        except:\n",
    "            failed_mapping_urls.append(url)\n",
    "            failed_mapping_messages.append('The parsing failed')\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "    done_count = len(urls) - len(failed_mapping_urls)\n",
    "    success_rate = str(round(100*done_count / len(texts),2))\n",
    "\n",
    "    print('Parsed and Saved ' + str(done_count) + ' Texts (' + success_rate + '% Success)' )\n",
    "\n",
    "    failed_urls = failed_mapping_urls + failed_scraping_urls\n",
    "    failed_messages = failed_mapping_messages + failed_scraping_messages\n",
    "\n",
    "    failed_dict = { 'Url': failed_urls,\n",
    "                    'Message': failed_messages\n",
    "    }\n",
    "\n",
    "    df_failed = pd.DataFrame.from_dict(failed_dict)\n",
    "    failed_subfolder_path = 'Scraper Results\\Failed Reports'\n",
    "    failed_path = os.path.join(directory_path,failed_subfolder_path)\n",
    "    df_failed.to_csv(failed_path + '\\Failed_Report_' +str(len(failed_mapping_urls))+ '.csv', encoding='utf-8-sig')\n",
    "\n",
    "#Combine All Methods\n",
    "\n",
    "def parliament_scraper (url1,url2, explicit_wait_seconds, poll_frequency,directory_path,chrome_driver_path):\n",
    "\n",
    "    if path_exists(directory_path) == 'true':\n",
    "        urls = url_list(url1,url2)\n",
    "        scraper_dict = scraper(urls,explicit_wait_seconds,poll_frequency,chrome_driver_path)\n",
    "        parser(scraper_dict,directory_path)\n",
    "    else:\n",
    "        print('Invalid folder directory provided. Check directory_path variable')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory_path = r'C:\\Users\\ivank\\Desktop'\n",
    "chrome_driver_path = r'C:\\Program Files (x86)\\chromedriver-win64\\chromedriver.exe'\n",
    "\n",
    "url1 = 'https://www.parliament.bg/bg/plenaryst/ns/55/ID/10602'\n",
    "url2 = 'https://www.parliament.bg/bg/plenaryst/ns/55/ID/10729'\n",
    "\n",
    "explicit_wait_seconds = 10\n",
    "poll_frequency = 2\n",
    "\n",
    "\n",
    "parliament_scraper(url1, url2, explicit_wait_seconds, poll_frequency,directory_path,chrome_driver_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of URLs to scrape: 9\n",
      "Number of scraped URLs: 9 (100.0% Success)\n"
     ]
    }
   ],
   "source": [
    "directory_path = r'C:\\Users\\ivank\\Desktop'\n",
    "chrome_driver_path = r'C:\\Program Files (x86)\\chromedriver-win64\\chromedriver.exe'\n",
    "\n",
    "url1 = 'https://www.parliament.bg/bg/plenaryst/ns/55/ID/10602'\n",
    "url2 = 'https://www.parliament.bg/bg/plenaryst/ns/55/ID/10610'\n",
    "\n",
    "explicit_wait_seconds = 30\n",
    "poll_frequency = 4\n",
    "\n",
    "urls = url_list(url1,url2)\n",
    "\n",
    "scraper_dict = scraper (urls,explicit_wait_seconds,poll_frequency,chrome_driver_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = scraper_dict.get('texts')\n",
    "urls = scraper_dict.get('successful_urls')\n",
    "failed_scraping_urls = scraper_dict.get('unsuccessful_urls')\n",
    "failed_scraping_messages = scraper_dict.get('unsuccessful_messages')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
