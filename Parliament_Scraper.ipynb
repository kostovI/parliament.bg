{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0.0 Packages Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.0 Define Parameters\n",
    "\n",
    "### 1.1 Define a function to create a List of target URLs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def url_list(url1,url2=None):\n",
    "\n",
    "    url_base = 'https://www.parliament.bg/bg/plenaryst/ns/55/ID/'\n",
    "\n",
    "    if url2 == None:\n",
    "        print('Number of URLs to scrape: 1')\n",
    "        urls = [url1]\n",
    "        return urls\n",
    "\n",
    "    else:\n",
    "\n",
    "        urls = []\n",
    "        url1_num = int(url1.split('/')[8])\n",
    "        url2_num = int(url2.split('/')[8])\n",
    "\n",
    "        while url1_num < url2_num + 1:\n",
    "            url_combined = url_base + str(url1_num)\n",
    "            urls.append(url_combined)\n",
    "            url1_num +=1\n",
    "\n",
    "        print('Number of URLs to scrape: ' + str(len(urls)))\n",
    "        return urls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.0 Scraping with Selenium\n",
    "\n",
    "### 2.1 Use Regular Expressions to identify the end of a Parliament Hearing (will be used as criteria for a successful extraction of a text body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Regular Expressions to parse different Elements from Corpus\n",
    "\n",
    "def end_of_hearing(text):\n",
    "    end_of_hearing_pattern = re.compile(r'\\d\\d,\\d\\d\\sч.\\)\\n{2,4}[А-Я][а-я]+')\n",
    "    match = end_of_hearing_pattern.search(text)\n",
    "    return match\n",
    "\n",
    "def end_position_hearing(text):\n",
    "    end_of_hearing_pattern = re.compile(r'\\d\\d,\\d\\d\\sч.\\)\\n{2,4}[А-Я][а-я]+')\n",
    "    match = end_of_hearing_pattern.search(text)\n",
    "    end_position = match.start()\n",
    "    return end_position"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Set up Chrome Driver "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chrome_driver ():\n",
    "\n",
    "    options = webdriver.ChromeOptions()\n",
    "\n",
    "    #Optimize the the driver\n",
    "    options.add_argument(\"--headless=new\")          # Run headless (no GUI)\n",
    "    options.add_argument(\"--no-sandbox\")            # Disable sandbox for WSL/Docker\n",
    "    options.add_argument(\"--disable-dev-shm-usage\") # Avoid shared memory crashes in WSL/Docker\n",
    "\n",
    "    driver = webdriver.Chrome(options=options)\n",
    "\n",
    "    return driver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Define Scraping Strategy and Conditions, create a dict with texts and error messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setup Crawler with an Explicit waiting strategy Fetch the corpus and map it to a dict\n",
    "\n",
    "def scraper (urls,explicit_wait_seconds=10,poll_frequency=2):\n",
    "\n",
    "    driver = chrome_driver()\n",
    "\n",
    "    texts = []\n",
    "    successful_urls = []\n",
    "    unsuccessful_urls = []\n",
    "    unsuccessful_messages = []\n",
    "\n",
    "    for url in tqdm(urls,desc='Scraping Hearings'):\n",
    "        time.sleep(0.1)\n",
    "\n",
    "        driver.get(url)\n",
    "\n",
    "        try:\n",
    "            WebDriverWait(driver, explicit_wait_seconds,poll_frequency).until(EC.presence_of_element_located((By.XPATH, '/html/body/div/main/div/div/div[2]/div[1]/div/div[3]')))\n",
    "            corpus = driver.find_element(By.ID, 'app')\n",
    "\n",
    "            if end_of_hearing(corpus.text) is None:\n",
    "                unsuccessful_urls.append(url)\n",
    "                unsuccessful_messages.append('Initial Xpath located for Url but corpus is empty')\n",
    "\n",
    "            else:\n",
    "                successful_urls.append(url)\n",
    "                texts.append(corpus.text)\n",
    "\n",
    "        except:\n",
    "            WebDriverWait(driver, explicit_wait_seconds, poll_frequency).until(EC.presence_of_element_located((By.XPATH, '//*[@id=\"app\"]/main/div/div/div[2]/div[1]/div/div[2]')))\n",
    "            corpus = driver.find_element(By.ID, 'app')\n",
    "\n",
    "            if end_of_hearing(corpus.text) is None:\n",
    "\n",
    "                unsuccessful_urls.append(url)\n",
    "                unsuccessful_messages.append('Error triggered: Initial Xpath was not located and no corpus was found')\n",
    "\n",
    "            else:\n",
    "\n",
    "                successful_urls.append(url)\n",
    "                texts.append(corpus.text)\n",
    "\n",
    "    scraper_dict = {\n",
    "        'texts' : texts,\n",
    "        'successful_urls' : successful_urls,\n",
    "        'unsuccessful_urls' : unsuccessful_urls,\n",
    "        'unsuccessful_messages' : unsuccessful_messages\n",
    "    }\n",
    "\n",
    "    print('Number of successfully scraped URLs: ' + str(len(successful_urls)) + ' (' + str(round(100*len(successful_urls)/len(urls),2)) +'%)' )\n",
    "\n",
    "    return scraper_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.0 Parsing texts with Regular Expressions\n",
    "### 3.1 Creation of Regular Expressions patterns for identification of attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get Basic Attributes, which will be added to each statement (Assembly Number, Hearing Number, Date)\n",
    "assembly_pattern = re.compile(r'[А-Я]+\\sИ\\s[А-Я]+\\sНАРОДНО\\sСЪБРАНИЕ|[А-Я]+\\sНАРОДНО\\sСЪБРАНИЕ')\n",
    "session_pattern = re.compile(r'[А-Я]+\\sИ\\s[А-Я]+\\sСЕСИЯ|[А-Я]+\\sСЕСИЯ')\n",
    "hearing_pattern = re.compile(r'([А-Я]+\\sИ\\s[А-Я]+\\s[А-Я]+\\sЗАСЕДАНИЕ|[А-Я]+\\sИ\\s[А-Я]+\\sЗАСЕДАНИЕ|[А-Я]+\\s[А-Я]+\\sЗАСЕДАНИЕ|[А-Я]+\\sЗАСЕДАНИЕ)')\n",
    "\n",
    "#find  first name + last name + political party + statement\n",
    "pattern_statements = re.compile(r'([А-Я]+\\s)?([А-Я]+\\s)([А-Я]+)(:|\\s\\((.+)\\):)')\n",
    "\n",
    "# find the date of the session\n",
    "pattern_date = re.compile(r'(\\d{2}).(\\d{2}).(\\d{4})')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Create Functions to Extract Information about Hearing (general and statement specific)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to extract general attributes for a hearing from a text body\n",
    "def general_parser (text):\n",
    "\n",
    "    assembly_matches = assembly_pattern.search(text)\n",
    "\n",
    "    try:\n",
    "        session_matches = session_pattern.search(text)\n",
    "    except:\n",
    "        session_matches = 'False'\n",
    "\n",
    "    if session_matches == 'False':\n",
    "\n",
    "        hearing_matches = hearing_pattern.search(text[session_matches.end():])\n",
    "    else:\n",
    "        hearing_matches = hearing_pattern.search(text[assembly_matches.end():])\n",
    "\n",
    "\n",
    "    assembly = assembly_matches.group().title()\n",
    "    hearing = hearing_matches.group().title()\n",
    "\n",
    "    matches_date = pattern_date.search(text)\n",
    "    year = matches_date.group(3)\n",
    "    month = matches_date.group(2)\n",
    "    day = matches_date.group(1)\n",
    "    date = year + '.' + month + '.' + day\n",
    "\n",
    "    general_info_dict = {\n",
    "        'assembly': assembly,\n",
    "        'hearing': hearing,\n",
    "        'date': date\n",
    "                         }\n",
    "\n",
    "    return general_info_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get Statements, politicians first and last names and political parties\n",
    "def statements_parser (text,url):\n",
    "\n",
    "    match_position = pattern_statements.finditer(text)\n",
    "    general_info_dict = general_parser(text)\n",
    "\n",
    "    first_names = []\n",
    "    last_names = []\n",
    "    political_parties_raw = []\n",
    "    assembly_roles = []\n",
    "    statements = []\n",
    "    start_positions_politician = []\n",
    "    end_positions_politician = []\n",
    "    assemblies = []\n",
    "    hearings = []\n",
    "    dates = []\n",
    "    urls = []\n",
    "\n",
    "    for index in match_position:\n",
    "\n",
    "        first_names.append(index.group(2).title())\n",
    "        last_names.append(index.group(3).title())\n",
    "        political_parties_raw.append(index.group(4))\n",
    "        end_positions_politician.append(index.end())\n",
    "        start_positions_politician.append(index.start())\n",
    "\n",
    "        if index.group(1) is None:\n",
    "            assembly_roles.append('Политик')\n",
    "        else:\n",
    "            assembly_roles.append(index.group(1).title())\n",
    "\n",
    "    number_statements = len(first_names)\n",
    "\n",
    "    i=0\n",
    "\n",
    "    while  i < number_statements:\n",
    "\n",
    "        assemblies.append(general_info_dict.get('assembly'))\n",
    "        hearings.append(general_info_dict.get('hearing'))\n",
    "        dates.append(general_info_dict.get('date'))\n",
    "        urls.append(url)\n",
    "        i+=1\n",
    "\n",
    "\n",
    "    start_position_statement = []\n",
    "    end_position_statement = []\n",
    "    last_hearing_position = end_position_hearing(text)\n",
    "\n",
    "    i = 0\n",
    "\n",
    "    while i < number_statements :\n",
    "\n",
    "        if i == len(first_names) - 1:\n",
    "            start_position_statement.append(end_positions_politician[i])\n",
    "            end_position_statement.append(last_hearing_position)\n",
    "\n",
    "            statement = text[end_positions_politician[i]:last_hearing_position]\n",
    "            clean_statement = statement.translate({ord(i): None for i in '('}).replace('\\n', ' ').strip()\n",
    "            statements.append(clean_statement)\n",
    "\n",
    "        else:\n",
    "            start_position_statement.append(end_positions_politician[i])\n",
    "            end_position_statement.append(start_positions_politician[i+1])\n",
    "\n",
    "            statement = text[end_positions_politician[i]:start_positions_politician[i+1]]\n",
    "            clean_statement = statement.replace('\\n', ' ').strip()\n",
    "            statements.append(clean_statement)\n",
    "\n",
    "        i+=1\n",
    "\n",
    "    political_parties = []\n",
    "    speaking_locations = []\n",
    "\n",
    "    for party in political_parties_raw:\n",
    "\n",
    "        if party == ':':\n",
    "            political_parties.append('Председателски Орган')\n",
    "            speaking_locations.append('От Трибуната')\n",
    "        else:\n",
    "            if ', от' in party:\n",
    "                clean = party.translate({ord(i): None for i in '():'}).strip()\n",
    "                clean_split = clean.split(', ')\n",
    "                political_parties.append(clean_split[0])\n",
    "                speaking_locations.append(clean_split[1].title())\n",
    "\n",
    "            elif 'встрани от микрофоните' in party:\n",
    "                speaking_locations.append('От Място')\n",
    "                political_parties.append('')\n",
    "\n",
    "            else:\n",
    "                clean = party.translate({ord(i): None for i in '():'}).strip()\n",
    "                political_parties.append(clean)\n",
    "                speaking_locations.append('От Трибуната')\n",
    "\n",
    "\n",
    "    statements_dict = {\n",
    "        'Народно Събрание': assemblies,\n",
    "        'Заседание': hearings,\n",
    "        'Дата': dates,\n",
    "        'Позиция в Парламента': assembly_roles,\n",
    "        'Първо Име': first_names,\n",
    "        'Фамилно Име': last_names,\n",
    "        'Партия': political_parties,\n",
    "        'Говорил От': speaking_locations,\n",
    "        'Изказване': statements,\n",
    "        'Начална Позиция на Изказване': start_position_statement,\n",
    "        'Крайна Позиция на Изказване': end_position_statement,\n",
    "        'Линк към изказване': urls\n",
    "    }\n",
    "\n",
    "    return statements_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of URLs to scrape: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping Hearings: 100%|██████████| 1/1 [00:14<00:00, 14.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of successfully scraped URLs: 1 (100.0%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "url1 = 'https://parliament.bg/bg/plenaryst/ns/55/ID/10969'\n",
    "url2 = 'https://parliament.bg/bg/plenaryst/ns/55/ID/11003'\n",
    "\n",
    "urls = url_list(url1)\n",
    "scraper_dict = scraper(urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Народно Събрание</th>\n",
       "      <th>Заседание</th>\n",
       "      <th>Дата</th>\n",
       "      <th>Позиция в Парламента</th>\n",
       "      <th>Първо Име</th>\n",
       "      <th>Фамилно Име</th>\n",
       "      <th>Партия</th>\n",
       "      <th>Говорил От</th>\n",
       "      <th>Изказване</th>\n",
       "      <th>Начална Позиция на Изказване</th>\n",
       "      <th>Крайна Позиция на Изказване</th>\n",
       "      <th>Линк към изказване</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Петдесет И Първо Народно Събрание</td>\n",
       "      <td>Деветнадесето Заседание</td>\n",
       "      <td>2025.01.31</td>\n",
       "      <td>Председател</td>\n",
       "      <td>Наталия</td>\n",
       "      <td>Киселова</td>\n",
       "      <td>Председателски Орган</td>\n",
       "      <td>От Трибуната</td>\n",
       "      <td>Добър ден, колеги! Моля за регистрация. Регист...</td>\n",
       "      <td>588</td>\n",
       "      <td>1882</td>\n",
       "      <td>https://parliament.bg/bg/plenaryst/ns/55/ID/10969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Петдесет И Първо Народно Събрание</td>\n",
       "      <td>Деветнадесето Заседание</td>\n",
       "      <td>2025.01.31</td>\n",
       "      <td>Политик</td>\n",
       "      <td>Атанас</td>\n",
       "      <td>Атанасов</td>\n",
       "      <td>ПП-ДБ</td>\n",
       "      <td>От Трибуната</td>\n",
       "      <td>Благодаря Ви, госпожо Председател. Уважаеми да...</td>\n",
       "      <td>1906</td>\n",
       "      <td>4145</td>\n",
       "      <td>https://parliament.bg/bg/plenaryst/ns/55/ID/10969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Петдесет И Първо Народно Събрание</td>\n",
       "      <td>Деветнадесето Заседание</td>\n",
       "      <td>2025.01.31</td>\n",
       "      <td>Политик</td>\n",
       "      <td>Пп</td>\n",
       "      <td>Меч</td>\n",
       "      <td>Председателски Орган</td>\n",
       "      <td>От Трибуната</td>\n",
       "      <td>„Заради такива комунисти като теб!“) Демократи...</td>\n",
       "      <td>4152</td>\n",
       "      <td>6340</td>\n",
       "      <td>https://parliament.bg/bg/plenaryst/ns/55/ID/10969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Петдесет И Първо Народно Събрание</td>\n",
       "      <td>Деветнадесето Заседание</td>\n",
       "      <td>2025.01.31</td>\n",
       "      <td>Председател</td>\n",
       "      <td>Наталия</td>\n",
       "      <td>Киселова</td>\n",
       "      <td>Председателски Орган</td>\n",
       "      <td>От Трибуната</td>\n",
       "      <td>За декларация от името на група – господин Дур...</td>\n",
       "      <td>6369</td>\n",
       "      <td>6433</td>\n",
       "      <td>https://parliament.bg/bg/plenaryst/ns/55/ID/10969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Петдесет И Първо Народно Събрание</td>\n",
       "      <td>Деветнадесето Заседание</td>\n",
       "      <td>2025.01.31</td>\n",
       "      <td>Политик</td>\n",
       "      <td>Галин</td>\n",
       "      <td>Дурев</td>\n",
       "      <td>БСП – Обединена левица</td>\n",
       "      <td>От Трибуната</td>\n",
       "      <td>Уважаема госпожо Председател, уважаеми госпожи...</td>\n",
       "      <td>6470</td>\n",
       "      <td>8474</td>\n",
       "      <td>https://parliament.bg/bg/plenaryst/ns/55/ID/10969</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Народно Събрание                Заседание        Дата  \\\n",
       "0  Петдесет И Първо Народно Събрание  Деветнадесето Заседание  2025.01.31   \n",
       "1  Петдесет И Първо Народно Събрание  Деветнадесето Заседание  2025.01.31   \n",
       "2  Петдесет И Първо Народно Събрание  Деветнадесето Заседание  2025.01.31   \n",
       "3  Петдесет И Първо Народно Събрание  Деветнадесето Заседание  2025.01.31   \n",
       "4  Петдесет И Първо Народно Събрание  Деветнадесето Заседание  2025.01.31   \n",
       "\n",
       "  Позиция в Парламента Първо Име Фамилно Име                  Партия  \\\n",
       "0         Председател   Наталия     Киселова    Председателски Орган   \n",
       "1              Политик   Атанас     Атанасов                   ПП-ДБ   \n",
       "2              Политик       Пп          Меч    Председателски Орган   \n",
       "3         Председател   Наталия     Киселова    Председателски Орган   \n",
       "4              Политик    Галин        Дурев  БСП – Обединена левица   \n",
       "\n",
       "     Говорил От                                          Изказване  \\\n",
       "0  От Трибуната  Добър ден, колеги! Моля за регистрация. Регист...   \n",
       "1  От Трибуната  Благодаря Ви, госпожо Председател. Уважаеми да...   \n",
       "2  От Трибуната  „Заради такива комунисти като теб!“) Демократи...   \n",
       "3  От Трибуната  За декларация от името на група – господин Дур...   \n",
       "4  От Трибуната  Уважаема госпожо Председател, уважаеми госпожи...   \n",
       "\n",
       "   Начална Позиция на Изказване  Крайна Позиция на Изказване  \\\n",
       "0                           588                         1882   \n",
       "1                          1906                         4145   \n",
       "2                          4152                         6340   \n",
       "3                          6369                         6433   \n",
       "4                          6470                         8474   \n",
       "\n",
       "                                  Линк към изказване  \n",
       "0  https://parliament.bg/bg/plenaryst/ns/55/ID/10969  \n",
       "1  https://parliament.bg/bg/plenaryst/ns/55/ID/10969  \n",
       "2  https://parliament.bg/bg/plenaryst/ns/55/ID/10969  \n",
       "3  https://parliament.bg/bg/plenaryst/ns/55/ID/10969  \n",
       "4  https://parliament.bg/bg/plenaryst/ns/55/ID/10969  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = scraper_dict.get('texts')[0]\n",
    "url = urls[0]\n",
    "\n",
    "general_parser_dict = general_parser(text)\n",
    "statements_parser_dict = statements_parser(text,url)\n",
    "\n",
    "hearing_df = pd.DataFrame(statements_parser_dict)\n",
    "\n",
    "hearing_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if raw_data directory exists and if not create one then create inside hearings folder and failed reports\n",
    "def create_directories ():\n",
    "\n",
    "    current_working_directory = os.getcwd()\n",
    "    raw_data_dir = current_working_directory + '/raw_data'\n",
    "\n",
    "    #Create Raw Data Directory\n",
    "    if os.path.exists(raw_data_dir):\n",
    "        pass\n",
    "    else:\n",
    "        print(\"Raw Data Directory created.\")\n",
    "        os.mkdir(raw_data_dir)\n",
    "\n",
    "    os.chdir(raw_data_dir)\n",
    "\n",
    "    #Create Inside directory Hearings and Failed Reports\n",
    "    try:\n",
    "        os.mkdir('Failed Reports')\n",
    "        os.mkdir('Hearings')\n",
    "    except FileExistsError:\n",
    "        pass\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "    os.chdir(current_working_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: CWD no longer exists - please use %cd to change directory.\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create subfolder for each assembly\n",
    "\n",
    "def sub_folder_creator (assembly,directory_path):\n",
    "\n",
    "    hearings_path = os.path.join(directory_path, 'Scraper Results','Hearings')\n",
    "    os.chdir(hearings_path)\n",
    "\n",
    "    try:\n",
    "        os.mkdir(assembly)\n",
    "        print(f\"Directory '{hearings_path}\\{assembly}' successfully created.\")\n",
    "    except FileExistsError:\n",
    "        pass\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "def save_df (statements_dict, directory_path):\n",
    "\n",
    "    df = pd.DataFrame.from_dict(statements_dict)\n",
    "    folder_path = 'Scraper Results\\Hearings'\n",
    "    save_path = os.path.join(directory_path, folder_path,statements_dict.get('Народно Събрание')[0])\n",
    "    df.to_csv(save_path +'\\{date}.csv'.format(date= statements_dict.get('Дата')[0]), encoding='utf-8-sig')\n",
    "\n",
    "#Create a function Iterating through texts and mapping texts to CSV and subsequently saving them\n",
    "\n",
    "def parser (scraper_dict,directory_path):\n",
    "\n",
    "    texts = scraper_dict.get('texts')\n",
    "    urls = scraper_dict.get('successful_urls')\n",
    "    failed_scraping_urls = scraper_dict.get('unsuccessful_urls')\n",
    "    failed_scraping_messages = scraper_dict.get('unsuccessful_messages')\n",
    "\n",
    "\n",
    "    failed_mapping_urls = []\n",
    "    failed_mapping_messages = []\n",
    "    main_folder_creator (directory_path)\n",
    "\n",
    "    for text,url in zip(texts,urls):\n",
    "\n",
    "        try:\n",
    "            statements_dict = statements_parser(text,url)\n",
    "            assembly = statements_dict.get('Народно Събрание')[0]\n",
    "            sub_folder_creator(assembly,directory_path)\n",
    "            save_df(statements_dict,directory_path)\n",
    "\n",
    "        except:\n",
    "            failed_mapping_urls.append(url)\n",
    "            failed_mapping_messages.append('The parsing failed')\n",
    "\n",
    "\n",
    "    done_count = len(urls) - len(failed_mapping_urls)\n",
    "    success_rate = str(round(100*done_count / len(texts),2))\n",
    "\n",
    "    print('Parsed and Saved ' + str(done_count) + ' Texts (' + success_rate + '% Success)' )\n",
    "\n",
    "    failed_urls = failed_mapping_urls + failed_scraping_urls\n",
    "    failed_messages = failed_mapping_messages + failed_scraping_messages\n",
    "\n",
    "    failed_dict = { 'Url': failed_urls,\n",
    "                    'Message': failed_messages\n",
    "    }\n",
    "\n",
    "    df_failed = pd.DataFrame.from_dict(failed_dict)\n",
    "    failed_subfolder_path = 'Scraper Results\\Failed Reports'\n",
    "    failed_path = os.path.join(directory_path,failed_subfolder_path)\n",
    "    df_failed.to_csv(failed_path + '\\Failed_Report_' +str(len(failed_mapping_urls))+ '.csv', encoding='utf-8-sig')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Combine All Methods\n",
    "\n",
    "def parliament_scraper (url1,url2,directory_path):\n",
    "\n",
    "    if path_exists(directory_path) == 'true':\n",
    "        urls = url_list(url1,url2)\n",
    "        scraper_dict = scraper(urls)\n",
    "        parser(scraper_dict,directory_path)\n",
    "    else:\n",
    "        print('Invalid folder directory provided. Check directory_path variable')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function which combines all previous Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invalid folder directory provided. Check directory_path variable\n"
     ]
    }
   ],
   "source": [
    "directory_path = r'C:\\Users\\ivank\\Desktop'\n",
    "\n",
    "url1 = 'https://www.parliament.bg/bg/plenaryst/ns/55/ID/10602'\n",
    "url2 = 'https://www.parliament.bg/bg/plenaryst/ns/55/ID/10729'\n",
    "\n",
    "explicit_wait_seconds = 10\n",
    "poll_frequency = 2\n",
    "\n",
    "\n",
    "parliament_scraper(url1, url2, directory_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "parliament.bg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
