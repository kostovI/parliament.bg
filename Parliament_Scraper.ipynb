{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0.0 Packages Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.0 Define Parameters\n",
    "\n",
    "### 1.1 Define a function to create a List of target URLs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def url_list(url1,url2=None):\n",
    "\n",
    "    url_base = 'https://www.parliament.bg/bg/plenaryst/ns/55/ID/'\n",
    "\n",
    "    if url2 == None:\n",
    "        print('Number of URLs to scrape: 1')\n",
    "        urls = [url1]\n",
    "        return urls\n",
    "\n",
    "    else:\n",
    "\n",
    "        urls = []\n",
    "        url1_num = int(url1.split('/')[8])\n",
    "        url2_num = int(url2.split('/')[8])\n",
    "\n",
    "        while url1_num < url2_num + 1:\n",
    "            url_combined = url_base + str(url1_num)\n",
    "            urls.append(url_combined)\n",
    "            url1_num +=1\n",
    "\n",
    "        print('Number of URLs to scrape: ' + str(len(urls)))\n",
    "        return urls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.0 Scraping with Selenium\n",
    "\n",
    "### 2.1 Use Regular Expressions to identify the end of a Parliament Hearing (will be used as criteria for a successful extraction of a text body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Regular Expressions to parse different Elements from Corpus\n",
    "\n",
    "def end_of_hearing(text):\n",
    "    end_of_hearing_pattern = re.compile(r'\\d\\d,\\d\\d\\sч.\\)\\n{2,4}[А-Я][а-я]+')\n",
    "    match = end_of_hearing_pattern.search(text)\n",
    "    return match\n",
    "\n",
    "def end_position_hearing(text):\n",
    "    end_of_hearing_pattern = re.compile(r'\\d\\d,\\d\\d\\sч.\\)\\n{2,4}[А-Я][а-я]+')\n",
    "    match = end_of_hearing_pattern.search(text)\n",
    "    end_position = match.start()\n",
    "    return end_position"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Set up Chrome Driver "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chrome_driver ():\n",
    "\n",
    "    options = webdriver.ChromeOptions()\n",
    "\n",
    "    #Optimize the the driver\n",
    "    options.add_argument(\"--headless=new\")          # Run headless (no GUI)\n",
    "    options.add_argument(\"--no-sandbox\")            # Disable sandbox for WSL/Docker\n",
    "    options.add_argument(\"--disable-dev-shm-usage\") # Avoid shared memory crashes in WSL/Docker\n",
    "\n",
    "    driver = webdriver.Chrome(options=options)\n",
    "\n",
    "    return driver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Define Scraping Strategy and Conditions, create a dict with texts and error messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setup Crawler with an Explicit waiting strategy Fetch the corpus and map it to a dict\n",
    "\n",
    "def scraper (urls,explicit_wait_seconds=10,poll_frequency=2):\n",
    "\n",
    "    driver = chrome_driver()\n",
    "\n",
    "    texts = []\n",
    "    successful_urls = []\n",
    "    unsuccessful_urls = []\n",
    "    unsuccessful_messages = []\n",
    "\n",
    "    for url in tqdm(urls,desc='Scraping Hearings'):\n",
    "        time.sleep(0.1)\n",
    "\n",
    "        driver.get(url)\n",
    "\n",
    "        try:\n",
    "            WebDriverWait(driver, explicit_wait_seconds,poll_frequency).until(EC.presence_of_element_located((By.XPATH, '/html/body/div/main/div/div/div[2]/div[1]/div/div[3]')))\n",
    "            corpus = driver.find_element(By.ID, 'app')\n",
    "\n",
    "            if end_of_hearing(corpus.text) is None:\n",
    "                unsuccessful_urls.append(url)\n",
    "                unsuccessful_messages.append('Initial Xpath located for Url but corpus is empty')\n",
    "\n",
    "            else:\n",
    "                successful_urls.append(url)\n",
    "                texts.append(corpus.text)\n",
    "\n",
    "        except:\n",
    "            WebDriverWait(driver, explicit_wait_seconds, poll_frequency).until(EC.presence_of_element_located((By.XPATH, '//*[@id=\"app\"]/main/div/div/div[2]/div[1]/div/div[2]')))\n",
    "            corpus = driver.find_element(By.ID, 'app')\n",
    "\n",
    "            if end_of_hearing(corpus.text) is None:\n",
    "\n",
    "                unsuccessful_urls.append(url)\n",
    "                unsuccessful_messages.append('Error triggered: Initial Xpath was not located and no corpus was found')\n",
    "\n",
    "            else:\n",
    "\n",
    "                successful_urls.append(url)\n",
    "                texts.append(corpus.text)\n",
    "\n",
    "    scraper_dict = {\n",
    "        'texts' : texts,\n",
    "        'successful_urls' : successful_urls,\n",
    "        'unsuccessful_urls' : unsuccessful_urls,\n",
    "        'unsuccessful_messages' : unsuccessful_messages\n",
    "    }\n",
    "\n",
    "    print('Number of successfully scraped URLs: ' + str(len(successful_urls)) + ' (' + str(round(100*len(successful_urls)/len(urls),2)) +'%)' )\n",
    "\n",
    "    return scraper_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of URLs to scrape: 21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping Hearings: 100%|██████████| 21/21 [02:05<00:00,  5.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of scraped URLs: 21 (100.0% Success)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "url1 = 'https://www.parliament.bg/bg/plenaryst/ns/55/ID/10602'\n",
    "url2 = 'https://www.parliament.bg/bg/plenaryst/ns/55/ID/10622'\n",
    "\n",
    "urls = url_list(url1,url2)\n",
    "scraper_dict = scraper(urls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.0 Parsing texts with Regular Expressions\n",
    "### 3.1 Creation of Regular Expressions patterns for identification of attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get Basic Attributes, which will be added to each statement (Assembly Number, Hearing Number, Date)\n",
    "assembly_pattern = re.compile(r'[А-Я]+\\sИ\\s[А-Я]+\\sНАРОДНО\\sСЪБРАНИЕ|[А-Я]+\\sНАРОДНО\\sСЪБРАНИЕ')\n",
    "session_pattern = re.compile(r'[А-Я]+\\sИ\\s[А-Я]+\\sСЕСИЯ|[А-Я]+\\sСЕСИЯ')\n",
    "hearing_pattern = re.compile(r'([А-Я]+\\sИ\\s[А-Я]+\\s[А-Я]+\\sЗАСЕДАНИЕ|[А-Я]+\\sИ\\s[А-Я]+\\sЗАСЕДАНИЕ|[А-Я]+\\s[А-Я]+\\sЗАСЕДАНИЕ|[А-Я]+\\sЗАСЕДАНИЕ)')\n",
    "\n",
    "#find  first name + last name + political party + statement\n",
    "pattern_statements = re.compile(r'([А-Я]+\\s)?([А-Я]+\\s)([А-Я]+)(:|\\s\\((.+)\\):)')\n",
    "\n",
    "# find the date of the session\n",
    "pattern_date = re.compile(r'(\\d{2}).(\\d{2}).(\\d{4})')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Create Functions to Extract Information about Hearing (general and statement specific)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to extract general attributes for a hearing from a text body\n",
    "def general_parser (text):\n",
    "\n",
    "    assembly_matches = assembly_pattern.search(text)\n",
    "\n",
    "    try:\n",
    "        session_matches = session_pattern.search(text)\n",
    "    except:\n",
    "        session_matches = 'False'\n",
    "\n",
    "\n",
    "    if session_matches == 'False':\n",
    "\n",
    "        hearing_matches = hearing_pattern.search(text[session_matches.end():])\n",
    "    else:\n",
    "        hearing_matches = hearing_pattern.search(text[assembly_matches.end():])\n",
    "\n",
    "\n",
    "    assembly = assembly_matches.group().title()\n",
    "    hearing = hearing_matches.group().title()\n",
    "\n",
    "\n",
    "    matches_date = pattern_date.search(text)\n",
    "    year = matches_date.group(3)\n",
    "    month = matches_date.group(2)\n",
    "    day = matches_date.group(1)\n",
    "    date = year + '.' + month + '.' + day\n",
    "\n",
    "    general_info_dict = {\n",
    "\n",
    "        'assembly': assembly,\n",
    "        'hearing': hearing,\n",
    "        'date': date\n",
    "                         }\n",
    "\n",
    "    return general_info_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Get Statements, politicians first and last names and political parties\n",
    "def statements_parser (text,url):\n",
    "\n",
    "\n",
    "    match_position = pattern_statements.finditer(text)\n",
    "    general_info_dict = general_parser(text)\n",
    "\n",
    "    first_names = []\n",
    "    last_names = []\n",
    "    political_parties_raw = []\n",
    "    assembly_roles = []\n",
    "    statements = []\n",
    "    start_positions_politician = []\n",
    "    end_positions_politician = []\n",
    "    assemblies = []\n",
    "    hearings = []\n",
    "    dates = []\n",
    "    urls = []\n",
    "\n",
    "    for index in match_position:\n",
    "\n",
    "        first_names.append(index.group(2).title())\n",
    "        last_names.append(index.group(3).title())\n",
    "        political_parties_raw.append(index.group(4))\n",
    "        end_positions_politician.append(index.end())\n",
    "        start_positions_politician.append(index.start())\n",
    "\n",
    "        if index.group(1) is None:\n",
    "            assembly_roles.append('Политик')\n",
    "        else:\n",
    "            assembly_roles.append(index.group(1).title())\n",
    "\n",
    "    number_statements = len(first_names)\n",
    "\n",
    "    i=0\n",
    "\n",
    "    while  i < number_statements:\n",
    "\n",
    "        assemblies.append(general_info_dict.get('assembly'))\n",
    "        hearings.append(general_info_dict.get('hearing'))\n",
    "        dates.append(general_info_dict.get('date'))\n",
    "        urls.append(url)\n",
    "        i+=1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    start_position_statement = []\n",
    "    end_position_statement = []\n",
    "    last_hearing_position = end_position_hearing(text)\n",
    "\n",
    "    i = 0\n",
    "\n",
    "    while i < number_statements :\n",
    "\n",
    "        if i == len(first_names) - 1:\n",
    "            start_position_statement.append(end_positions_politician[i])\n",
    "            end_position_statement.append(last_hearing_position)\n",
    "\n",
    "            statement = text[end_positions_politician[i]:last_hearing_position]\n",
    "            clean_statement = statement.translate({ord(i): None for i in '('}).replace('\\n', ' ').strip()\n",
    "            statements.append(clean_statement)\n",
    "\n",
    "        else:\n",
    "            start_position_statement.append(end_positions_politician[i])\n",
    "            end_position_statement.append(start_positions_politician[i+1])\n",
    "\n",
    "            statement = text[end_positions_politician[i]:start_positions_politician[i+1]]\n",
    "            clean_statement = statement.replace('\\n', ' ').strip()\n",
    "            statements.append(clean_statement)\n",
    "\n",
    "        i+=1\n",
    "\n",
    "    political_parties = []\n",
    "    speaking_locations = []\n",
    "\n",
    "    for party in political_parties_raw:\n",
    "\n",
    "        if party == ':':\n",
    "            political_parties.append('Председателски Орган')\n",
    "            speaking_locations.append('От Трибуната')\n",
    "        else:\n",
    "            if ', от' in party:\n",
    "                clean = party.translate({ord(i): None for i in '():'}).strip()\n",
    "                clean_split = clean.split(', ')\n",
    "                political_parties.append(clean_split[0])\n",
    "                speaking_locations.append(clean_split[1].title())\n",
    "\n",
    "            elif 'встрани от микрофоните' in party:\n",
    "                speaking_locations.append('От Място')\n",
    "                political_parties.append('')\n",
    "\n",
    "            else:\n",
    "                clean = party.translate({ord(i): None for i in '():'}).strip()\n",
    "                political_parties.append(clean)\n",
    "                speaking_locations.append('От Трибуната')\n",
    "\n",
    "\n",
    "    statements_dict = {\n",
    "        'Народно Събрание': assemblies,\n",
    "        'Заседание': hearings,\n",
    "        'Дата': dates,\n",
    "        'Позиция в Парламента': assembly_roles,\n",
    "        'Първо Име': first_names,\n",
    "        'Фамилно Име': last_names,\n",
    "        'Партия': political_parties,\n",
    "        'Говорил От': speaking_locations,\n",
    "        'Изказване': statements,\n",
    "        'Начална Позиция на Изказване': start_position_statement,\n",
    "        'Крайна Позиция на Изказване': end_position_statement,\n",
    "        'Линк към изказване': urls\n",
    "    }\n",
    "\n",
    "    return statements_dict\n",
    "\n",
    "#Create Subfolders and save the ready CSV Files there\n",
    "\n",
    "def path_exists (path):\n",
    "    try:\n",
    "        os.chdir(path)\n",
    "        return 'true'\n",
    "    except:\n",
    "        return 'The specified directory path does not exist!'\n",
    "\n",
    "def main_folder_creator (directory_path):\n",
    "\n",
    "    directory_name = 'Scraper Results'\n",
    "\n",
    "    #create main folder directory\n",
    "    os.chdir(directory_path)\n",
    "\n",
    "    try:\n",
    "        os.mkdir(directory_name)\n",
    "        print(f\"Directory of scraper Results'{directory_path}\\{directory_name}' successfully created.\")\n",
    "    except FileExistsError:\n",
    "        pass\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "    #create subfolders Hearings and Failed Reports\n",
    "    folders_directory_path = os.path.join(directory_path, directory_name)\n",
    "    os.chdir(folders_directory_path)\n",
    "\n",
    "    try:\n",
    "        os.mkdir('Failed Reports')\n",
    "        os.mkdir('Hearings')\n",
    "    except FileExistsError:\n",
    "        pass\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "#create subfolder for each assembly\n",
    "\n",
    "def sub_folder_creator (assembly,directory_path):\n",
    "\n",
    "    hearings_path = os.path.join(directory_path, 'Scraper Results','Hearings')\n",
    "    os.chdir(hearings_path)\n",
    "\n",
    "    try:\n",
    "        os.mkdir(assembly)\n",
    "        print(f\"Directory '{hearings_path}\\{assembly}' successfully created.\")\n",
    "    except FileExistsError:\n",
    "        pass\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "def save_df (statements_dict, directory_path):\n",
    "\n",
    "    df = pd.DataFrame.from_dict(statements_dict)\n",
    "    folder_path = 'Scraper Results\\Hearings'\n",
    "    save_path = os.path.join(directory_path, folder_path,statements_dict.get('Народно Събрание')[0])\n",
    "    df.to_csv(save_path +'\\{date}.csv'.format(date= statements_dict.get('Дата')[0]), encoding='utf-8-sig')\n",
    "\n",
    "#Create a function Iterating through texts and mapping texts to CSV and subsequently saving them\n",
    "\n",
    "def parser (scraper_dict,directory_path):\n",
    "\n",
    "    texts = scraper_dict.get('texts')\n",
    "    urls = scraper_dict.get('successful_urls')\n",
    "    failed_scraping_urls = scraper_dict.get('unsuccessful_urls')\n",
    "    failed_scraping_messages = scraper_dict.get('unsuccessful_messages')\n",
    "\n",
    "\n",
    "    failed_mapping_urls = []\n",
    "    failed_mapping_messages = []\n",
    "    main_folder_creator (directory_path)\n",
    "\n",
    "    for text,url in zip(texts,urls):\n",
    "\n",
    "        try:\n",
    "            statements_dict = statements_parser(text,url)\n",
    "            assembly = statements_dict.get('Народно Събрание')[0]\n",
    "            sub_folder_creator(assembly,directory_path)\n",
    "            save_df(statements_dict,directory_path)\n",
    "\n",
    "        except:\n",
    "            failed_mapping_urls.append(url)\n",
    "            failed_mapping_messages.append('The parsing failed')\n",
    "\n",
    "\n",
    "    done_count = len(urls) - len(failed_mapping_urls)\n",
    "    success_rate = str(round(100*done_count / len(texts),2))\n",
    "\n",
    "    print('Parsed and Saved ' + str(done_count) + ' Texts (' + success_rate + '% Success)' )\n",
    "\n",
    "    failed_urls = failed_mapping_urls + failed_scraping_urls\n",
    "    failed_messages = failed_mapping_messages + failed_scraping_messages\n",
    "\n",
    "    failed_dict = { 'Url': failed_urls,\n",
    "                    'Message': failed_messages\n",
    "    }\n",
    "\n",
    "    df_failed = pd.DataFrame.from_dict(failed_dict)\n",
    "    failed_subfolder_path = 'Scraper Results\\Failed Reports'\n",
    "    failed_path = os.path.join(directory_path,failed_subfolder_path)\n",
    "    df_failed.to_csv(failed_path + '\\Failed_Report_' +str(len(failed_mapping_urls))+ '.csv', encoding='utf-8-sig')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Combine All Methods\n",
    "\n",
    "def parliament_scraper (url1,url2,directory_path):\n",
    "\n",
    "    if path_exists(directory_path) == 'true':\n",
    "        urls = url_list(url1,url2)\n",
    "        scraper_dict = scraper(urls)\n",
    "        parser(scraper_dict,directory_path)\n",
    "    else:\n",
    "        print('Invalid folder directory provided. Check directory_path variable')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function which combines all previous Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invalid folder directory provided. Check directory_path variable\n"
     ]
    }
   ],
   "source": [
    "directory_path = r'C:\\Users\\ivank\\Desktop'\n",
    "\n",
    "url1 = 'https://www.parliament.bg/bg/plenaryst/ns/55/ID/10602'\n",
    "url2 = 'https://www.parliament.bg/bg/plenaryst/ns/55/ID/10729'\n",
    "\n",
    "explicit_wait_seconds = 10\n",
    "poll_frequency = 2\n",
    "\n",
    "\n",
    "parliament_scraper(url1, url2, directory_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "parliament.bg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
